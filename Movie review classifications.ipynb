{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, movie_reviews\n",
    "import sklearn\n",
    "import random\n",
    "\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "stop_words = stopwords.words()\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "all_words = [item for item in all_words if (len(item)>2 or item not in stop_words)]\n",
    "word_features = list(all_words)[:2000] [1]\n",
    "\n",
    "def document_features(document):\n",
    "    words = [item for item in document if (len(item)>2 or item not in stop_words)]\n",
    "    document_words = set(words)\n",
    "\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raws = [(movie_reviews.raw(fileid), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(raws)\n",
    "\n",
    "print(raws[0][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc= 0.818970\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def decode_cat(cat):\n",
    "    if 'pos' in cat:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "x = [row[0] for row in raws]\n",
    "y = [decode_cat(row[1]) for row in raws]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend('film movie just time story character life'.split())\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_doc(doc):\n",
    "    clean = re.sub('[^0-9a-zA-Z]+', ' ', doc)\n",
    "    words = clean.lower().split()\n",
    "    result = []\n",
    "    for word in words:\n",
    "        afterl = lemmatiser.lemmatize(word)\n",
    "        if (len(afterl)>3 and afterl not in stop_words):\n",
    "            result.append(afterl)\n",
    "    return result\n",
    "\n",
    "    \n",
    "clf = MultinomialNB()\n",
    "countvec = CountVectorizer(analyzer=preprocess_doc, max_df=0.9, min_df=4)\n",
    "MNB_clf = Pipeline([('vect', countvec),  ('tfidf', TfidfTransformer()), ('clf', clf), ])\n",
    "\n",
    "scores = cross_val_score(MNB_clf, x, y)\n",
    "print(\"Acc= %f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: like even scene good make would much first well action also plot really know thing year could little take star\n",
      "Topic #1: like make scene even love good much performance also year would well take work play comedy come best role people\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "countvec = CountVectorizer(analyzer=preprocess_doc, max_df=0.9, min_df=4)\n",
    "x_after_vec = countvec.fit_transform(x)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=2)\n",
    "lda.fit(x_after_vec)\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "feature_names = countvec.get_feature_names()\n",
    "print_top_words(lda, feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc= 0.742004\n"
     ]
    }
   ],
   "source": [
    "clf = SVC()\n",
    "SVC_clf = Pipeline([('vect', CountVectorizer()),  ('tfidf', TfidfTransformer()), ('clf', clf), ])\n",
    "\n",
    "scores = cross_val_score(SVC_clf, x, y)\n",
    "print(\"Acc= %f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc= 0.647484\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "random_clf = Pipeline([('vect', CountVectorizer()),  ('tfidf', TfidfTransformer()), ('clf', clf), ])\n",
    "\n",
    "scores = cross_val_score(random_clf, x, y)\n",
    "print(\"Acc= %f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc= 0.819510\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier()\n",
    "countvec = CountVectorizer(stop_words='english')\n",
    "nn_clf = Pipeline([('vect', countvec),  ('tfidf', TfidfTransformer()), ('clf', clf), ])\n",
    "\n",
    "scores = cross_val_score(nn_clf, x, y)\n",
    "print(\"Acc= %f\" % scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movies Reviews\n",
    "## CountVectorizer\n",
    "- Using CountVectorizer and TF-IDF improve performance than using simple nltk example method and classifier\n",
    "- But using n-grams make decrease accuracy\n",
    "- Removing Stop words barely improve accuracy\n",
    "## Classifiers\n",
    "- Random Forest: Least accuracy but fast though => 0.65\n",
    "- SVM classifier: Slow but better than random forest => 0.74\n",
    "- AdaBoostClassifier: Slow and not very good accuracy => 0.75\n",
    "- GradientBoostingClassifier: Slow but good accuracy => 0.78\n",
    "- MultinomialNB and LogisticRegression: Best performance with high accuracy => 0.82\n",
    "- DNN MLPClassifier: Slow but slighty higher accuracy => 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc= 0.781995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100)\n",
    "countvec = CountVectorizer(stop_words='english')\n",
    "gb_clf = Pipeline([('vect', countvec),  ('tfidf', TfidfTransformer()), ('clf', clf), ])\n",
    "\n",
    "scores = cross_val_score(gb_clf, x, y)\n",
    "print(\"Acc= %f\" % scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc= 0.753000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "countvec = CountVectorizer(stop_words='english')\n",
    "ad_clf = Pipeline([('vect', countvec),  ('tfidf', TfidfTransformer()), ('clf', clf), ])\n",
    "\n",
    "scores = cross_val_score(ad_clf, x, y)\n",
    "print(\"Acc= %f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc= 0.844004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "countvec = CountVectorizer(analyzer=preprocess_doc, max_df=0.9, min_df=4)\n",
    "ad_clf = Pipeline([('vect', countvec),  ('tfidf', TfidfTransformer()), ('clf', clf), ])\n",
    "\n",
    "scores = cross_val_score(ad_clf, x, y)\n",
    "print(\"Acc= %f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/25\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 0.6934 - acc: 0.4856 - val_loss: 0.6932 - val_acc: 0.4925\n",
      "Epoch 2/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6933 - acc: 0.4813 - val_loss: 0.6932 - val_acc: 0.4925\n",
      "Epoch 3/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6933 - acc: 0.4894 - val_loss: 0.6933 - val_acc: 0.4925\n",
      "Epoch 4/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6932 - acc: 0.5019 - val_loss: 0.6932 - val_acc: 0.4925\n",
      "Epoch 5/25\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 0.6931 - acc: 0.5025 - val_loss: 0.6931 - val_acc: 0.5075\n",
      "Epoch 6/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6933 - acc: 0.4931 - val_loss: 0.6932 - val_acc: 0.4925\n",
      "Epoch 7/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6932 - acc: 0.5019 - val_loss: 0.6932 - val_acc: 0.4925\n",
      "Epoch 8/25\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 0.6932 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5400\n",
      "Epoch 9/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6933 - acc: 0.4875 - val_loss: 0.6932 - val_acc: 0.4925\n",
      "Epoch 10/25\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 0.6932 - acc: 0.4850 - val_loss: 0.6931 - val_acc: 0.4925\n",
      "Epoch 11/25\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 0.6931 - acc: 0.5062 - val_loss: 0.6930 - val_acc: 0.5350\n",
      "Epoch 12/25\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 0.6931 - acc: 0.5162 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 13/25\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 0.6931 - acc: 0.5044 - val_loss: 0.6931 - val_acc: 0.4950\n",
      "Epoch 14/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6932 - acc: 0.5050 - val_loss: 0.6930 - val_acc: 0.4950\n",
      "Epoch 15/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6933 - acc: 0.4944 - val_loss: 0.6933 - val_acc: 0.4925\n",
      "Epoch 16/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6932 - acc: 0.4988 - val_loss: 0.6930 - val_acc: 0.5325\n",
      "Epoch 17/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6931 - acc: 0.5162 - val_loss: 0.6931 - val_acc: 0.4925\n",
      "Epoch 18/25\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 0.6931 - acc: 0.5038 - val_loss: 0.6930 - val_acc: 0.4900\n",
      "Epoch 19/25\n",
      "1600/1600 [==============================] - 16s 10ms/step - loss: 0.6931 - acc: 0.5144 - val_loss: 0.6930 - val_acc: 0.5350\n",
      "Epoch 20/25\n",
      "1600/1600 [==============================] - 15s 9ms/step - loss: 0.6930 - acc: 0.5162 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "Epoch 21/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6931 - acc: 0.5081 - val_loss: 0.6930 - val_acc: 0.4975\n",
      "Epoch 22/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6930 - acc: 0.5150 - val_loss: 0.6930 - val_acc: 0.4925\n",
      "Epoch 23/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6931 - acc: 0.5181 - val_loss: 0.6930 - val_acc: 0.4925\n",
      "Epoch 24/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6931 - acc: 0.5044 - val_loss: 0.6929 - val_acc: 0.5125\n",
      "Epoch 25/25\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.6931 - acc: 0.5175 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "400/400 [==============================] - 1s 2ms/step\n",
      "[0.69296298265457157, 0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=preprocess_doc, max_df=0.9)\n",
    "x_train_counts = count_vect.fit_transform(x).toarray()\n",
    "\n",
    "input_dim = np.max(x_train_counts) + 1\n",
    "embedding_dims = 20\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_counts, y, test_size=0.2)\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=25)\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
